#Intall python 3.5
#Add to list of interpreters in Pycharm https://www.jetbrains.com/help/pycharm-edu/adding-existing-virtual-environment.html
#Switch to it
#Install ethnicolr in pycharm settings
#For geopanda to work, need to install GDAL, Fiona, pyproj, six, shapely - see http://geopandas.org/install.html
#Get them from https://www.lfd.uci.edu/~gohlke/pythonlibs/, put the wheels in src\Python37\venv\Scripts and run pip on them
#Never managed to get ESRI File GDB write drivers to work: https://gis.stackexchange.com/questions/193288/how-to-add-support-for-filegdb-esri-file-gdb-api-driver-in-fiona to get FileGDB writing support

#Got sandbox (for dbf conversion) through http://geodasandbox.github.io/2012/03/28/how-to-get-up-and-running-with-the-geoda-center-sandbox/
#and transferred pyGDsandbox to \\src\\Python37
#from pyGDsandbox.dataIO import df2dbf, dbf2df

import pandas as pd
import geopandas as gpd
import numpy as np
import ethnicolr
import os
import re
import feather
import collections
from floodvulne35.floodvulne35.panda_to_csvforArcGIS import panda_to_csv

pd.set_option('display.max_columns', 500)


rootdir = 'C:\Mathis\ICSL\\flood_vulnerability'
resdir = os.path.join(rootdir,'results')
os.chdir(resdir)
gdb = 'flood_risk.gdb'
gdb_vulne = 'flood_vulnerability.gdb'

#Function to run weighted average on all numeric columns in a df
def weighed_average(grp, column='PARCELPOP'): #From https://stackoverflow.com/questions/10951341/pandas-dataframe-aggregate-function-using-multiple-columns
    return (grp._get_numeric_data().multiply(grp[column], axis=0).sum())/grp[column].sum()

residindiv = pd.read_csv('parcel_taxrollname_residindiv.csv',
                         usecols=['PolyID','TaxRollID','party1_last1', 'party1_first1'],
                         dtype = {'PolyID': np.object, 'TaxRollID': np.object})
#residindiv was not filtered for duplicate TaxRollID (l 349 of flood_risk_WA_format.py) so filter here,
#otherwise, leads to some records having more names analyzed than taxroll records associated with a parcel
polychecksub = gpd.read_file(gdb_vulne, layer='parcel_taxrollname_residential_polycheck_sub')
residindiv = residindiv.merge(polychecksub, how='inner', on='TaxRollID')
residindiv = residindiv.drop(columns='PolyID_y')
residindiv = residindiv.rename(index=str, columns={"PolyID_x": "PolyID"})

residindiv_full = residindiv[residindiv['party1_first1'].notnull() & residindiv['party1_last1'].notnull()]
feather.write_dataframe(residindiv, os.path.join(resdir,'residindiv_filter.feather'))

parcelblock = pd.read_csv('parcels_blocksfill_subfield.csv', low_memory=False,
                          dtype = {'OBJECTID': np.object, 'PolyID': np.object, 'BLOCKID10': np.object,
                                   'GEOID10_bg': np.object, 'GEOID10_t': np.object})
blockgroup = gpd.read_file(gdb, layer='blockgroup2010_proj')
tract = gpd.read_file(gdb, layer='tracts_WAproj')

#-----------------------------  PREDICT RACE/ETHINICITY OF INDIVIDUAL LAND OWNERS BASED ON THEIR NAME ------------------
#CENSUS 2010 LAST NAME DATABASE to predict race and ethnicity of individual land owners
# rdf_ln2010 = ethnicolr.census_ln(residindiv, 'party1_last1', 2010)
# rdf_ln2010.replace('(S)', 0, inplace=True)
# for c in rdf_ln2010.columns[6:]:
#     rdf_ln2010[c] = pd.to_numeric(rdf_ln2010[c])/100
# rdf_ln2010.loc[rdf_ln2010['party1_last1'].isnull(),6:rdf_ln2010.shape[1]] = np.NaN #Correct glitch in ethnicolr that models race even if last name is NaN
# rdf_ln2010.to_csv(os.path.join(resdir,'parcel_taxrollname_residindiv_census_ln.csv'))
rdf_ln2010 = pd.read_csv(os.path.join(resdir, 'parcel_taxrollname_residindiv_census_ln.csv'),
                      dtype={'PolyID': np.object, 'TaxRollID': np.object, 'pctwhite': np.float64, 'pctblack': np.float64,
                             'pctapi': np.float64, 'pctaian': np.float64, 'pct2race':np.float64, 'pcthispanic':np.float64
                             })

#CENSUS 2010 LAST NAME model to predict the race and ethnicity of individual land owners
# rdfmod_ln2010 = ethnicolr.pred_census_ln(residindiv, 'party1_last1', 2010)
# rdfmod_ln2010.replace('(S)', 0, inplace=True)
# rdfmod_ln2010.loc[rdfmod_ln2010['party1_last1'].isnull(),6:rdfmod_ln2010.shape[1]] = np.NaN #Glitch in ethnicolr that models race even if last name is NaN
# rdfmod_ln2010.to_csv(os.path.join(resdir,'parcel_taxrollname_residindiv_pred_census_ln.csv'))
rdfmod_ln2010 = pd.read_csv(os.path.join(resdir, 'parcel_taxrollname_residindiv_pred_census_ln.csv'),
                     dtype={'PolyID': np.object, 'TaxRollID': np.object})

#WIKIPEDIA LAST NAME model to predict the race and ethnicity of individual land owners
# wikilndf = ethnicolr.pred_wiki_ln(residindiv, 'party1_last1')
# wikilndf.replace('(S)', 0, inplace=True)
# wikilndf.columns = [c.split(',')[-1] for c in wikilndf2010.columns] #Keep only most precise group for race/ethnicity to remove commas and shorten column names
# wikilndf.columns = list(wikilndf2010.columns[:6])+[c[:5]+'_wln' for c in wikilndf2010.columns[6:]]
# wikilndf.loc[wikilndf['party1_last1'].isnull(),6:wikilndf.shape[1]] = np.NaN #Glitch in ethnicolr that models race even if last name is NaN
# wikilndf2010.to_csv(os.path.join(resdir,'parcel_taxrollname_residindiv_pred_wiki_ln.csv'))
wikilndf2010 = pd.read_csv(os.path.join(resdir, 'parcel_taxrollname_residindiv_pred_wiki_ln.csv'),
                         dtype = {'PolyID':np.object, 'TaxRollID': np.object})

#WIKIPEDIA FULL NAME model to predict the race and ethnicity of individual land owners
# wikidf2010 = ethnicolr.pred_wiki_name(residindiv_full, 'party1_last1', 'party1_first1')
# wikidf2010.replace('(S)', 0, inplace=True)
# wikidf2010.columns = [c.split(',')[-1] for c in wikidf2010.columns] #Keep only most precise group for race/ethnicity to remove commas and shorten column names
# wikidf2010.columns = list(wikidf2010.columns[:6])+[c[:5]+'_wf' for c in wikidf2010.columns[6:]]
# wikidf2010.to_csv(os.path.join(resdir,'parcel_taxrollname_residindiv_pred_wiki_name.csv'))
wikidf2010 = pd.read_csv(os.path.join(resdir, 'parcel_taxrollname_residindiv_pred_wiki_name.csv'),
                         dtype = {'PolyID':np.object, 'TaxRollID': np.object})

#FLORIDA REGISTRATION FULL NAME model to predict the race and ethnicity of individual land owners
# flregdf2010 = ethnicolr.pred_fl_reg_name(residindiv_full, 'party1_last1', 'party1_first1')
# flregdf2010.replace('(S)', 0, inplace=True)
# flregdf2010.columns = list(flregdf2010.columns[:6])+[c+'fl' for c in flregdf2010.columns[6:]]
# flregdf2010.to_csv(os.path.join(resdir,'parcel_taxrollname_residindiv_pred_flreg_name.csv'))
flregdf2010 = pd.read_csv(os.path.join(resdir, 'parcel_taxrollname_residindiv_pred_flreg_name.csv'),
                         dtype = {'PolyID':np.object, 'TaxRollID': np.object})

#Merge all predictions together
#Join parcel info wikidf and rd
merge1a = rdfmod_ln2010.merge(rdf_ln2010, how ='outer', on = 'TaxRollID',
                       suffixes = ('_rdfmod', '_rdf'), copy = False)
merge1b = merge1a.merge(wikidf2010, how ='outer', on = 'TaxRollID',
                       suffixes = ('', '_wf'), copy = False)
merge1c = merge1b.merge(wikilndf2010, how ='outer', on = 'TaxRollID',
                       suffixes = ('', '_wln'), copy = False)
# merge1d = merge1c.merge(flreglndf2010, how ='outer', on = 'TaxRollID',
#                        suffixes = ('', '_flregln'), copy = False)
merge1 = merge1c.merge(flregdf2010, how ='outer', on = 'TaxRollID',
                       suffixes = ('', '_fl'), copy = False)

#Check that NaN make sense and that keep the right last name column with the least NAs
dropcols = [c for c in ['race_rdfmod', 'Unnamed: 0','Unnamed: 0_rdf', 'PolyID_rdf', 'party1_last1_rdf', 'party1_first1_rdf',
                        'geometry_rdf', 'race_rdf','PolyID_wf',
                        'party1_last1','party1_first1', 'geometry', 'race', 'race_wf', 'Unnamed: 0_wln', 'Unnamed: 0.1',
                        'PolyID_wln', 'PolyID',
                        'party1_last1_wln', 'party1_first1_wln', 'geometry_wln', 'race_wln', 'Unnamed: 0_fl',
                        'Unnamed: 0.1_fl', 'PolyID_fl', 'party1_last1_fl','party1_first1_fl', 'geometry_fl', 'race_fl']
            if c in list(merge1.columns)]
merge1 = merge1.drop(columns=dropcols)
merge1 = merge1.rename(index=str, columns = {'PolyID_rdfmod':'PolyID'})

#-----------------------------  AGGREGATE RACE/ETHINICITY AT PARCEL LEVEL ----------------------------------------------
#----- Remove duplicate owner names for a given parcel
merge1_nodupli = merge1[(~merge1.duplicated(subset=['PolyID', 'party1_last1_rdfmod', 'party1_first1_rdfmod'])) |
                        (merge1['party1_last1_rdfmod'].isnull())]
#----- Average predictions by parcel, count number of records by parcel, then write out to CSV export
meancols = merge1_nodupli.columns[6:]
nonumcol = merge1_nodupli.columns[6:][merge1_nodupli.dtypes[6:].eq('object')]
merge1_nodupli[nonumcol] = merge1_nodupli[nonumcol].apply(pd.to_numeric, errors='coerce')
statdic = collections.OrderedDict()
for c in meancols:
    statdic[c] = 'mean'
statdic['TaxRollID'] = 'count'
parcel_raceag = merge1_nodupli.groupby('PolyID').agg(statdic)
parcel_raceag = parcel_raceag.rename(index=str, columns={"TaxRollID": "TaxRollIDCount"})
cols = parcel_raceag.columns.tolist()
parcel_raceag = parcel_raceag[cols[-1:] + cols[:-1]]
parcel_raceag.index.name = 'PolyID'
parcel_raceag.reset_index(inplace=True)

#Then merge with parcel-block intersection population size census data
merge2 = parcel_raceag.merge(parcelblock, how ='outer', on = 'PolyID', suffixes = ('', '_parcel'), copy = False)
feather.write_dataframe(merge2, os.path.join(resdir,'parcels_blocksfill_racepred.feather'))
#merge2 = feather.read_dataframe(os.path.join(resdir,'parcels_blocksfill_racepred.feather'))
merge2['modsum'] = merge2 ['white'] + merge2 ['api'] + merge2 ['hispanic'] + merge2['black']
merge2['modsum_census'] = merge2['pctwhite'] + merge2['pctapi'] + merge2['pcthispanic'] + \
                                   merge2['pctblack'] + merge2['pctaian'] + merge2['pct2prace']
#Total ; 3,598,851; PARCELPOP > 0: 2,051,130; PARCELPOP > 0 AND TaxRollIDCount >= 1: 1,761,674

#Aggregate by PolyID for merging back with parcel dissolved polygons (as some parcels have population in multiple blocks)
catcols = list(parcel_raceag.columns) + ['OBJECTID', 'StateLandUseCD', 'BLOCKID10', 'GEOID10_bg', 'GEOID10_t']
parcel_raceag2 = merge2[merge2['PARCELPOP'].notnull()].sort_values('PARCELPOP', ascending=False)\
    .drop_duplicates('PolyID').sort_index()[catcols] #Only keep the blockgroup and tract information for the parcel section that has the most population

statdic2 = collections.OrderedDict()
for c in ['HOUSINGMIN', 'TAXROLLNUM','HOUSINGADJ', 'PARCELPOP']:
    statdic2[c] = 'sum'
parcel_raceag3 = merge2.groupby('PolyID').agg(statdic2).reset_index()
parcel_raceag4 = parcel_raceag3.merge(parcel_raceag2, how ='outer', on = 'PolyID', copy = False)
#panda_to_csv(parcel_raceag4, outdir='racepred', outfile= 'parcels_blocksfill_racepred.csv', overwrite=True)
feather.write_dataframe(parcel_raceag4, os.path.join(resdir,'parcels_blocksfill_ethnicolrpred.feather'))
#DBF doesn't work. Writes gibberish after a while df2dbf(parcel_raceag2, os.path.join(resdir, 'racepred2.dbf'))

#-----------------------------  AGGREGATE RACE/ETHINICITY AT BLOCKGROUP LEVEL ----------------------------------------------
#----- Aggregate by BLOCKGROUP, summing population columns and performing a weight average by parcel population on race
merge2_sub = merge2[(merge2['PARCELPOP']>0)]
sumcols = ['TaxRollIDCount', 'BLCKMAJ', 'HOUSINGADJ', 'PARCELPOP','TAXROLLNUM']
statdic = collections.OrderedDict()
for c in sumcols:
    statdic[c] = 'sum'
statdic['PARCELPOP'] = [statdic['PARCELPOP']] + ['mean']
bg_ag1 = merge2_sub.groupby('GEOID10_bg').agg(statdic)
bg_ag1.columns = ['_'.join(col).strip() for col in bg_ag1.columns.values]

#Run average on all race columns aside from those generated by census database model (as many more NaN in that one)
merge2_sub2 = merge2_sub[round(merge2_sub['modsum'],2)==1] #Make sure to only run average on parcels that have race prediction data
sub2_meancols = [c for c in list(meancols) if c not in list(rdf_ln2010.columns[4:])]
bg_ag2 = merge2_sub2[['GEOID10_bg','PARCELPOP']+list(sub2_meancols)].groupby('GEOID10_bg').apply(weighed_average)
#Run average on all base census model race columns
merge2_sub3 = merge2_sub[round(merge2_sub['modsum_census'],0)==1] #Make sure to only run average on parcels that have race prediction data
bg_ag3 = merge2_sub3[['GEOID10_bg','PARCELPOP']+list(rdf_ln2010.columns[6:])].groupby('GEOID10_bg').apply(weighed_average)

bg_ag4 = bg_ag1.merge(bg_ag2, how='outer', on = 'GEOID10_bg') #Merge two summary stat df
bg_ag = bg_ag4.merge(bg_ag3, how='outer', on = 'GEOID10_bg').reset_index()
bg_ag = bg_ag.drop(columns=['PARCELPOP_x', 'PARCELPOP_y'])

#Subset blockgroup columns to include only data on race before exporting as .shp can only have 216 fields
regx_bgrace = re.compile('B02|B03', re.IGNORECASE)
racecols_bg = []
for c in blockgroup.columns:
    if regx_bgrace.match(c):
        racecols_bg.append(c)
keepcols = ['GEOID10', 'Shape_Length', 'Shape_Area','geometry']
blockgroup_attri = blockgroup[keepcols+racecols_bg].merge(bg_ag, how='outer', left_on = 'GEOID10', right_on = 'GEOID10_bg')
blockgroup_attri.columns = [c.split(',')[-1] for c in blockgroup_attri.columns] #Keep only most precise group for race/ethnicity to remove commas and shorten column names
blockgroup_attri.to_file('blockgroup_racepred.shp')

#-----------------------------  AGGREGATE RACE/ETHINICITY AT CENSUS TRACT LEVEL ----------------------------------------------
t_ag1 = merge2_sub.groupby('GEOID10_t').agg(statdic)
t_ag1.columns = ['_'.join(col).strip() for col in t_ag1.columns.values]
#Run average on all race columns aside from those generated by census database model (as many more NaN in that one)
t_ag2 = merge2_sub2[['GEOID10_t','PARCELPOP']+list(sub2_meancols)].groupby('GEOID10_t').apply(weighed_average)
#Run average on all base census model race columns
t_ag3 = merge2_sub3[['GEOID10_t','PARCELPOP']+list(rdf_ln2010.columns[6:])].groupby('GEOID10_t').apply(weighed_average)

t_ag4 = t_ag1.merge(t_ag2, how='outer', on = 'GEOID10_t') #Merge two summary stat df
t_ag = t_ag4.merge(t_ag3, how='outer', on = 'GEOID10_t').reset_index()
t_ag = t_ag.drop(columns=['PARCELPOP_x', 'PARCELPOP_y'])

#Subset tract columns to include only data on race before exporting as .shp can only have 216 fields
regx_trace = re.compile('DP008|DP009|DP011', re.IGNORECASE)
racecols_t = []
for c in tract.columns:
    if regx_trace.match(c):
        racecols_t.append(c)
tract_attri = tract[keepcols+racecols_t].merge(t_ag, how='outer', left_on = 'GEOID10', right_on = 'GEOID10_t')
tract_attri.to_file('tract_racepred.shp')

#To do in the future:
    #- Only include those names for which parcel TaxRoll property address matches their mailing address to only keep those that
        # actually live in their place rather than owners of rental units
#For calibration:
    #- For those parcels with no race data, simply put it at probability from census tract level